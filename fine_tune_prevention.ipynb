{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558778b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 16:14:48.126859: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-06 16:14:48.434100: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-06 16:14:48.434193: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-06 16:14:48.435244: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-06 16:14:48.554877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-06 16:14:49.714270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import transformers\n",
    "from transformers import (Trainer, TrainingArguments, AutoTokenizer , AutoModel, pipeline, \n",
    "                          DataCollatorWithPadding, TrainerCallback, AutoModelForSequenceClassification, EvalPrediction)\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import utils\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             classification_report, confusion_matrix)\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09cc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting fixed seeds\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seeds(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40a821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_name, file_path, text_field, label_field, years, seq_len):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.loc[df['Fiscal Year'].isin(years)]\n",
    "        self.encoding = {}\n",
    "        for field, seq_len in zip(text_field, seq_len):\n",
    "            df[field].fillna('No text available', inplace = True)\n",
    "            self.texts = df[field].tolist()\n",
    "            if field == 'Specific Aims':\n",
    "                field_encoding = self.tokenizer(self.texts, \n",
    "                                    max_length=seq_len,\n",
    "                                    padding='max_length',\n",
    "                                    return_tensors='pt',\n",
    "                                    truncation=True,\n",
    "                                    truncation_strategy = 'longest_first')\n",
    "            else:\n",
    "                field_encoding = self.tokenizer(self.texts, \n",
    "                                    max_length=seq_len,\n",
    "                                    padding='max_length',\n",
    "                                    return_tensors='pt',\n",
    "                                    truncation=True)\n",
    "            if self.encoding == {}:\n",
    "                self.encoding = field_encoding\n",
    "            else:\n",
    "                for key, val in self.encoding.items():\n",
    "                    self.encoding[key] = torch.cat((self.encoding[key], field_encoding[key]), dim = 1)\n",
    "                    \n",
    "        if df[label_field].isna().any():\n",
    "        #df[label_field].fillna('No label available', inplace = True) \n",
    "            print('label not available')\n",
    "            \n",
    "        self.labels = df[label_field].replace({0: 1, 1: 0}).tolist()\n",
    "        self.appl_id = df['Appl ID'].tolist()\n",
    "        self.activity_code = df['Activity Code'].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            \n",
    "        item['Appl ID'] = torch.tensor(self.appl_id[idx])\n",
    "        item['Activity Code'] = self.activity_code[idx]\n",
    "        return item\n",
    "    \n",
    "    \n",
    "def compute_metrics_fn(p: EvalPrediction):\n",
    "        predictions = p.predictions\n",
    "        label_ids = p.label_ids\n",
    "\n",
    "        logits = predictions\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "        report = classification_report(label_ids, preds)\n",
    "        cm = confusion_matrix(label_ids, preds)\n",
    "        print(report)\n",
    "        print('confusion matrix: ', cm)\n",
    "\n",
    "        metrics = {}\n",
    "        accuracy = accuracy_score(label_ids, preds)\n",
    "        precision = precision_score(label_ids, preds)\n",
    "        recall = recall_score(label_ids, preds)\n",
    "        f1 = f1_score(label_ids, preds)\n",
    "\n",
    "        metrics['accuracy'] = accuracy\n",
    "        metrics['precision'] = precision\n",
    "        metrics['recall'] = recall\n",
    "        metrics['eval_f1'] = f1\n",
    "\n",
    "        return metrics\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b827c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedTrainer(Trainer):\n",
    "   \n",
    "    \n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, loss_fn = 'class_weight'):\n",
    "#         if loss_fn == 'class_weight':\n",
    "#             return self.compute_weight_adjusting_loss(model, inputs, return_outputs)\n",
    "#         elif loss_fn == 'focal':\n",
    "#             return self.compute_focal_loss(model, inputs, return_outputs)\n",
    "#         else:\n",
    "#             raise ValueError(\"Invalid loss function specified\")\n",
    "            \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        target = inputs.get('labels')\n",
    "        \n",
    "        labels = torch.unique(target)\n",
    "        \n",
    "        class_distribution = torch.zeros(len(labels))\n",
    "        \n",
    "        for i in labels:\n",
    "            class_count = torch.count_nonzero(target == i)\n",
    "            class_distribution[i] = class_count\n",
    "            \n",
    "        #weights = (target.shape[0] / class_distribution).to(model.device)\n",
    "        weights = torch.where(class_distribution > 0, target.shape[0] / class_distribution, torch.zeros_like(class_distribution)).to(model.device)\n",
    "\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss(weight = weights)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        prob = torch.nn.functional.softmax(logits, dim = -1)\n",
    "        \n",
    "        loss = criterion(prob.to(target.device), target)\n",
    "        #gradients = [p.grad for p in model.parameters() if p.grad is not None]\n",
    "        #print('Gradients are:', gradients)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "     \n",
    "    #focal loss\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         #0.8 for prevention and 0.2 for non-prevention\n",
    "#         alpha = [0.55, 0.45]\n",
    "#         gamma = 2\n",
    "#         target = inputs.get('labels')\n",
    "#         labels = torch.unique(target)\n",
    "        \n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get('logits')\n",
    "#         #print(logits)\n",
    "#         prob = torch.nn.functional.log_softmax(logits, dim = -1)\n",
    "        \n",
    "#         #print(prob, target)\n",
    "        \n",
    "#         logpt = prob.gather(1,target.view(-1, 1))\n",
    "#         logpt = logpt.view(-1)\n",
    "#         pt = logpt.exp()\n",
    "        \n",
    "#         alpha = torch.tensor(alpha, device = model.device)\n",
    "#         if alpha is not None:\n",
    "#             #if alpha.type()!=inputs.data.type():\n",
    "#                 #alpha = alpha.type_as(inputs.data)\n",
    "#             at = alpha.gather(0,target.data.view(-1))\n",
    "#             logpt = logpt * at\n",
    "\n",
    "#         focal_loss = -1 * (1-pt)**gamma * logpt\n",
    "#         loss = focal_loss.sum() / len(focal_loss)\n",
    "#         #gradients = [p.grad for p in model.parameters() if p.grad is not None]\n",
    "#         #print('Gradients are:', gradients)\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d92c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "class PreventionTrainer():\n",
    "    def __init__(self,  model_name, output_dir, batch_size, learning_rate, weight_decay, num_epochs, lr_scheduler,\n",
    "                run_name, resume_checkpoint):\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.run_name = f\"{run_name}_{self.learning_rate}\"\n",
    "        self.resume_checkpoint = resume_checkpoint\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.datacollator = DataCollatorWithPadding(tokenizer = self.tokenizer)\n",
    "        \n",
    "    def verify_initial_loss(self):\n",
    "        n_classes = self.model.config.num_labels\n",
    "        expected_initial_loss = - torch.log(torch.tensor(1.0 / n_classes))\n",
    "        \n",
    "        dummy_input = torch.randn(2, self.batch_size)\n",
    "        dummy_target = torch.zeros(1, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        loss = compute_loss(self.model, dummy_input, return_outputs=False)\n",
    "        \n",
    "        if torch.allclose(loss, expected_initial_loss):\n",
    "            print(\"Initial loss verification successful!\")\n",
    "        else:\n",
    "            print(\"Initial loss verification failed.\")\n",
    "            print(f\"Expected initial loss: {expected_initial_loss}, Computed initial loss: {loss}\")\n",
    "       \n",
    "        \n",
    "    def _create_training_args(self):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = self.output_dir,\n",
    "            evaluation_strategy = 'epoch',\n",
    "            per_device_train_batch_size = self.batch_size,\n",
    "            per_device_eval_batch_size = self.batch_size,\n",
    "            learning_rate = self.learning_rate,\n",
    "            weight_decay = self.weight_decay,\n",
    "            num_train_epochs = self.num_epochs,\n",
    "            #lr_scheduler_type = self.lr_scheduler,\n",
    "            log_level = 'debug',\n",
    "            logging_strategy = 'steps',\n",
    "            save_strategy = 'epoch',\n",
    "            save_total_limit = 1,\n",
    "            seed = 42,\n",
    "            fp16 = True,\n",
    "            run_name = self.run_name,\n",
    "            #label_names = ['prevention', 'non_prev'],\n",
    "            load_best_model_at_end = True,\n",
    "            metric_for_best_model = \"eval_f1\",\n",
    "            label_smoothing_factor = 0.2,\n",
    "            debug = \"underflow_overflow\",\n",
    "            optim = 'adamw_hf',\n",
    "            #group_by_length = True,\n",
    "            report_to = 'mlflow',\n",
    "            #set when needed\n",
    "            resume_from_checkpoint = self.resume_checkpoint,\n",
    "            gradient_checkpointing = True )\n",
    "        \n",
    "        return training_args\n",
    "        \n",
    "\n",
    "    \n",
    "    def train(self, train_dataset, eval_dataset):\n",
    "        with mlflow.start_run():\n",
    "            # Log training parameters\n",
    "            mlflow.log_param(\"learning_rate\", self.learning_rate)\n",
    "            mlflow.log_param(\"batch_size\", self.batch_size)\n",
    "            mlflow.log_param(\"num_epochs\", self.num_epochs)\n",
    "            print(\"===> Loading Model and Training Arguments...\")\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.model.to(device)\n",
    "\n",
    "            training_args = self._create_training_args()\n",
    "\n",
    "            print(\"===> Predicting...\")\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model = self.model,\n",
    "                args = training_args,\n",
    "                data_collator = self.datacollator,\n",
    "                train_dataset = train_dataset,\n",
    "                eval_dataset = eval_dataset,\n",
    "                compute_metrics = compute_metrics_fn)\n",
    "\n",
    "#             def track_gradients(module, grad_input, grad_output):\n",
    "#                 for param_name, grad in module.named_parameters():\n",
    "#                     if grad is not None:\n",
    "#                         print(f'Layer: {param_name}, Mean Grad: {grad.mean().item()}, Max Grad: {grad.max().item()}')\n",
    "\n",
    "            # Attach the hook to each module's gradient computation\n",
    "    #         for module in self.model.modules():\n",
    "    #             module.register_backward_hook(track_gradients)\n",
    "\n",
    "            trainer.train()\n",
    "       \n",
    "        mlflow.end_run()\n",
    "        \n",
    "        \n",
    "    def train_kfold(self, dataset):\n",
    "        print(\"===> Loading Model and Training Arguments...\")\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold = 1\n",
    "        for train_idx, eval_idx in kfold.split(dataset):\n",
    "            # Set the experiment path\n",
    "            experiment_path = \"BioLinkClassifier\"\n",
    "            # Set the experiment\n",
    "            mlflow.set_experiment(experiment_path)\n",
    "            with mlflow.start_run():\n",
    "                # Log training parameters\n",
    "                mlflow.log_param(\"learning_rate\", self.learning_rate)\n",
    "                mlflow.log_param(\"batch_size\", self.batch_size)\n",
    "                mlflow.log_param(\"num_epochs\", self.num_epochs)\n",
    "                print(f\"Training on Fold {fold}\")\n",
    "                train_fold = [dataset[i] for i in (train_idx)]\n",
    "                eval_fold = [dataset[i] for i in (eval_idx)]\n",
    "\n",
    "                training_args = self._create_training_args()\n",
    "\n",
    "                print(\"===> Predicting...\")\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    model = self.model,\n",
    "                    args = training_args,\n",
    "                    data_collator = self.datacollator,\n",
    "                    train_dataset = train_fold,\n",
    "                    eval_dataset = eval_fold,\n",
    "                    compute_metrics = compute_metrics_fn    \n",
    "\n",
    "                )\n",
    "\n",
    "                trainer.train()\n",
    "                fold += 1\n",
    "                #self.run_name += str(fold)\n",
    "\n",
    "            mlflow.end_run()\n",
    "            \n",
    "    def evaluate_heldout(self, test_dataset, model_checkpoint):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels = 2)\n",
    "\n",
    "        training_args = self._create_training_args()\n",
    "\n",
    "        print(\"===> Performance on held-out set...\")\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model = model,\n",
    "            args = training_args,\n",
    "            data_collator = self.datacollator,\n",
    "            eval_dataset = test_dataset,\n",
    "            compute_metrics = compute_metrics_fn)\n",
    "        \n",
    "        ModelOutput = namedtuple(\n",
    "                        \"ModelOutput\",\n",
    "                        [\"predictions\",  \n",
    "                            \"label_ids\",   \n",
    "                            \"metrics\" ]\n",
    "                    )\n",
    "        print('predictions on test set:')\n",
    "        logits = trainer.predict(test_dataset).predictions\n",
    "        probabilities = F.softmax(torch.tensor(logits), dim=1)\n",
    "        probabilities = probabilities.numpy()\n",
    "\n",
    "        df = pd.DataFrame(probabilities, columns=['0', '1'])\n",
    "        df['predicted_class'] = df.apply(lambda row: 0 if row['0'] > row['1'] else 1, axis=1)\n",
    "        df['Appl ID'] = [item[\"Appl ID\"].item() for item in test_dataset]\n",
    "        df['True Label'] = [item['labels'].item() for item in test_dataset]\n",
    "        df['Activity Code'] = [item[\"Activity Code\"] for item in test_dataset]\n",
    "        \n",
    "        df.to_csv('LM_predictions.csv', index = False)\n",
    "        print(trainer.predict(test_dataset).predictions)\n",
    "        \n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3583735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #prepare training dataset\n",
    "    train_dataset = CustomDataset('michiyasunaga/BioLinkBERT-base', \n",
    "                                  #12 activity code\n",
    "                                 #'ODP_FY12-21_funded_20230815_Joined.csv', \n",
    "                                  #'2021coded_2023-06-22.csv',\n",
    "                                  #all ACs\n",
    "                                  'ODP_fy1221-2023-11-01.csv',\n",
    "                                  ['Title', 'Abstract', 'Public Health Relevance'], \n",
    "                                  'f6otherorunclear', \n",
    "                                  [2017, 2018, 2019, 2020, 2021], \n",
    "                                  [35, 221, 256])\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "#     eval_dataset = CustomDataset('michiyasunaga/BioLinkBERT-base', \n",
    "#                                  'ODP_FY12-21_funded_20230815_Joined.csv', \n",
    "#                                   ['Title', 'Abstract', 'Specific Aims'], \n",
    "#                                   'f6otherorunclear', \n",
    "#                                   [2021], \n",
    "#                                   [35, 221, 256])\n",
    "\n",
    "    eval_dataset = CustomDataset('michiyasunaga/BioLinkBERT-base',\n",
    "                                 #'2021coded_12ac_2023-08-15.csv',\n",
    "                                 #\"2021coded_2023-06-22.csv\",\n",
    "                                 #'ODP_fy1221-2023-11-01.csv',\n",
    "                                      #4300 examples\n",
    "                                #'2022coded_2023-08-16.csv', \n",
    "                                      #7500 examples\n",
    "                                  #'FY2022_09-28-2023.csv',\n",
    "                                      #2022 all data with activity codes\n",
    "                                  'ODP_fy22-all_codes_2023-11-01.csv',\n",
    "                                 ['Title', 'Abstract', 'Public Health Relevance'], \n",
    "                                 #'f6otherorunclear', \n",
    "                                 'X 1 Not Codable',\n",
    "                                 [2022], \n",
    "                                 [35, 221, 256])\n",
    "    #eval_dataset, test_dataset = train_test_split(eval_test_dataset, test_size=0.5, random_state=42)\n",
    "    \n",
    "# # # #     train_dataset, eval_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "   \n",
    "\n",
    "       #set experiment params\n",
    "    #candidate params for 2022\n",
    "    model_name ='michiyasunaga/BioLinkBERT-base'\n",
    "    output_dir = 'prevention_finetuned_2020'\n",
    "    batch_size = 32\n",
    "    learning_rate = 3e-5\n",
    "    weight_decay = 1e-5\n",
    "    num_epochs = 5\n",
    "    lr_scheduler = 'linear'\n",
    "    run_name = 'prev_biolink_finetune_19_20'\n",
    "    resume_checkpoint = None\n",
    "        \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    obj = PreventionTrainer(model_name, output_dir, batch_size, learning_rate, weight_decay, num_epochs, lr_scheduler,run_name, resume_checkpoint)\n",
    "    \n",
    "    \n",
    "# #     #train without cross validation\n",
    "    obj.train(train_dataset = train_dataset, eval_dataset = eval_dataset)\n",
    "    \n",
    "    #preparing test dataset\n",
    "#     eval_test_dataset = CustomDataset('michiyasunaga/BioLinkBERT-base',\n",
    "#                                  #'2021coded_12ac_2023-08-15.csv',\n",
    "#                                  #\"2021coded_2023-06-22.csv\",\n",
    "#                                '2022coded_2023-08-16.csv', \n",
    "#                                  ['Title', 'Abstract', 'Specific Aims'], \n",
    "#                                  #'f6otherorunclear', \n",
    "#                                  'X 1 Not Codable',\n",
    "#                                  [2022], \n",
    "#                                  [35, 221, 256])\n",
    "    \n",
    "#     #evaluate performance on unseen test data\n",
    "#     test_dataset = CustomDataset('michiyasunaga/BioLinkBERT-base',\n",
    "#                                  #'2021coded_12ac_2023-08-15.csv',\n",
    "#                                  #\"2021coded_2023-06-22.csv\",\n",
    "#                                #'2022coded_2023-08-16.csv',\n",
    "#                                  'FY2022_09-28-2023.csv',\n",
    "#                                  ['Title', 'Abstract', 'Specific Aims'], \n",
    "#                                  #'f6otherorunclear', \n",
    "#                                  'X 1 Not Codable',\n",
    "#                                  [2022], \n",
    "#                                  [35, 221, 256])\n",
    "    obj.evaluate_heldout(eval_dataset, 'prevention_finetuned_2020/checkpoint-1448')\n",
    "\n",
    "    \n",
    "    #training with kfold\n",
    "#     dataset = CustomDataset('michiyasunaga/BioLinkBERT-base', \n",
    "#                              'ODP_FY12-21_funded_20230815_Joined.csv',\n",
    "#                             #'2022coded_2023-08-16.csv',\n",
    "#                             ['Title', 'Abstract', 'Specific Aims'], \n",
    "#                             'f6otherorunclear', \n",
    "#                             #'X 1 Not Codable',\n",
    "#                             [2021], \n",
    "#                             [35, 221, 256])\n",
    "    #split dataset into training and evalkuation for k-fold and a held-out set\n",
    "    \n",
    "    #train_eval_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    #training the model\n",
    "    #obj.train_kfold(dataset = train_eval_dataset)\n",
    "    #obj.evaluate_heldout(test_dataset, 'prevention_finetuned/checkpoint-60')\n",
    "#     test_dataset2 = CustomDataset('michiyasunaga/BioLinkBERT-base', \n",
    "#                                   '2022coded_2023-08-16.csv', \n",
    "#                                   ['Title', 'Abstract', 'Specific Aims'], \n",
    "#                                   'X 1 Not Codable', \n",
    "#                                   [2022], \n",
    "#                                   [35, 221, 256])\n",
    "    #obj.evaluate_heldout(test_dataset2, 'prevention_finetuned/checkpoint-60')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf8408fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/tokenizer_config.json\n",
      "loading file vocab.txt from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/elaheh/.cache/huggingface/hub/models--michiyasunaga--BioLinkBERT-base/snapshots/b71f5d70f063d1c8f1124070ce86f1ee463ca1fe/tokenizer_config.json\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loading Model and Training Arguments...\n",
      "===> Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 32\n",
      "/home/elaheh/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 28,794\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4,500\n",
      "  Number of trainable parameters = 108,234,242\n",
      "MLflow experiment_name=None, run_name=prev_biolink_finetune_19_20_3e-05, nested=False, tags=False\n",
      "/tmp/ipykernel_13014/1676292039.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Activity Code, Appl ID. If Activity Code, Appl ID are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/elaheh/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 2:03:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.737605</td>\n",
       "      <td>0.844450</td>\n",
       "      <td>0.659514</td>\n",
       "      <td>0.836672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>0.520224</td>\n",
       "      <td>0.724266</td>\n",
       "      <td>0.829956</td>\n",
       "      <td>0.628399</td>\n",
       "      <td>0.854648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.525356</td>\n",
       "      <td>0.724906</td>\n",
       "      <td>0.832640</td>\n",
       "      <td>0.635344</td>\n",
       "      <td>0.843862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.563805</td>\n",
       "      <td>0.720601</td>\n",
       "      <td>0.820158</td>\n",
       "      <td>0.606529</td>\n",
       "      <td>0.887519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.588790</td>\n",
       "      <td>0.711957</td>\n",
       "      <td>0.815058</td>\n",
       "      <td>0.600282</td>\n",
       "      <td>0.874679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7451\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Activity Code, Appl ID. If Activity Code, Appl ID are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89      5504\n",
      "           1       0.66      0.84      0.74      1947\n",
      "\n",
      "    accuracy                           0.84      7451\n",
      "   macro avg       0.80      0.84      0.81      7451\n",
      "weighted avg       0.86      0.84      0.85      7451\n",
      "\n",
      "confusion matrix:  [[4663  841]\n",
      " [ 318 1629]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prevention_finetuned_2020/checkpoint-900\n",
      "Configuration saved in prevention_finetuned_2020/checkpoint-900/config.json\n",
      "Model weights saved in prevention_finetuned_2020/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [prevention_finetuned_2020/checkpoint-546] due to args.save_total_limit\n",
      "/tmp/ipykernel_13014/1676292039.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
      "/home/elaheh/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7451\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Activity Code, Appl ID. If Activity Code, Appl ID are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.88      5504\n",
      "           1       0.63      0.85      0.72      1947\n",
      "\n",
      "    accuracy                           0.83      7451\n",
      "   macro avg       0.78      0.84      0.80      7451\n",
      "weighted avg       0.86      0.83      0.84      7451\n",
      "\n",
      "confusion matrix:  [[4520  984]\n",
      " [ 283 1664]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prevention_finetuned_2020/checkpoint-1800\n",
      "Configuration saved in prevention_finetuned_2020/checkpoint-1800/config.json\n",
      "Model weights saved in prevention_finetuned_2020/checkpoint-1800/pytorch_model.bin\n",
      "/tmp/ipykernel_13014/1676292039.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
      "/home/elaheh/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7451\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Activity Code, Appl ID. If Activity Code, Appl ID are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88      5504\n",
      "           1       0.64      0.84      0.72      1947\n",
      "\n",
      "    accuracy                           0.83      7451\n",
      "   macro avg       0.79      0.84      0.80      7451\n",
      "weighted avg       0.86      0.83      0.84      7451\n",
      "\n",
      "confusion matrix:  [[4561  943]\n",
      " [ 304 1643]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prevention_finetuned_2020/checkpoint-2700\n",
      "Configuration saved in prevention_finetuned_2020/checkpoint-2700/config.json\n",
      "Model weights saved in prevention_finetuned_2020/checkpoint-2700/pytorch_model.bin\n",
      "Deleting older checkpoint [prevention_finetuned_2020/checkpoint-1800] due to args.save_total_limit\n",
      "/tmp/ipykernel_13014/1676292039.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
      "/home/elaheh/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7451\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Activity Code, Appl ID. If Activity Code, Appl ID are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.80      0.87      5504\n",
      "           1       0.61      0.89      0.72      1947\n",
      "\n",
      "    accuracy                           0.82      7451\n",
      "   macro avg       0.78      0.84      0.79      7451\n",
      "weighted avg       0.86      0.82      0.83      7451\n",
      "\n",
      "confusion matrix:  [[4383 1121]\n",
      " [ 219 1728]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prevention_finetuned_2020/checkpoint-3600\n",
      "Configuration saved in prevention_finetuned_2020/checkpoint-3600/config.json\n",
      "Model weights saved in prevention_finetuned_2020/checkpoint-3600/pytorch_model.bin\n",
      "Deleting older checkpoint [prevention_finetuned_2020/checkpoint-2700] due to args.save_total_limit\n",
      "/tmp/ipykernel_13014/1676292039.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
      "/home/elaheh/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7451\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Activity Code, Appl ID. If Activity Code, Appl ID are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86      5504\n",
      "           1       0.60      0.87      0.71      1947\n",
      "\n",
      "    accuracy                           0.82      7451\n",
      "   macro avg       0.77      0.83      0.79      7451\n",
      "weighted avg       0.86      0.82      0.82      7451\n",
      "\n",
      "confusion matrix:  [[4370 1134]\n",
      " [ 244 1703]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prevention_finetuned_2020/checkpoint-4500\n",
      "Configuration saved in prevention_finetuned_2020/checkpoint-4500/config.json\n",
      "Model weights saved in prevention_finetuned_2020/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [prevention_finetuned_2020/checkpoint-3600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from prevention_finetuned_2020/checkpoint-900 (score: 0.7376047090785601).\n",
      "Deleting older checkpoint [prevention_finetuned_2020/checkpoint-4500] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "prevention_finetuned_2020/checkpoint-1448 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/prevention_finetuned_2020/checkpoint-1448/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:428\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    429\u001b[0m         path_or_repo_id,\n\u001b[1;32m    430\u001b[0m         filename,\n\u001b[1;32m    431\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    432\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    433\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    434\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    435\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    436\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    437\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    438\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    439\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    440\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1197\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1198\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1199\u001b[0m         timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1200\u001b[0m     )\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[0;32m-> 1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:291\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-654aa2ca-04729a041cd433f405f77e4d)\n\nRepository Not Found for url: https://huggingface.co/prevention_finetuned_2020/checkpoint-1448/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[22], line 92\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     obj\u001b[38;5;241m.\u001b[39mtrain(train_dataset \u001b[38;5;241m=\u001b[39m train_dataset, eval_dataset \u001b[38;5;241m=\u001b[39m eval_dataset)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m#preparing test dataset\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#     eval_test_dataset = CustomDataset('michiyasunaga/BioLinkBERT-base',\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#                                  #'2021coded_12ac_2023-08-15.csv',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#                                  [2022], \u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#                                  [35, 221, 256])\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     obj\u001b[38;5;241m.\u001b[39mevaluate_heldout(eval_dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprevention_finetuned_2020/checkpoint-1448\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 148\u001b[0m, in \u001b[0;36mPreventionTrainer.evaluate_heldout\u001b[0;34m(self, test_dataset, model_checkpoint)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_heldout\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_dataset, model_checkpoint):\n\u001b[1;32m    147\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint, num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    150\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_training_args()\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===> Performance on held-out set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:482\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 482\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    483\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    484\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    485\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    488\u001b[0m )\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1007\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   1006\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1007\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1008\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1009\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    622\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:675\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    676\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    677\u001b[0m         configuration_file,\n\u001b[1;32m    678\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    679\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    680\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    681\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    682\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    683\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    684\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    685\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    686\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    687\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:449\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: prevention_finetuned_2020/checkpoint-1448 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13d7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
